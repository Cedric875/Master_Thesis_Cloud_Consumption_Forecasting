{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb8bec9-4728-40a7-9e3b-2abaa3ff9ade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb91b7a-a0d8-49ac-acab-65aba762b9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/df_aggregated_month_populated_btp_core.parquet'\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "df_aggregated_month_populated_btp_core = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(df_aggregated_month_populated_btp_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8ca6a0-1078-4c49-ab88-8a3c0bfedaf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_aggregated_month_populated_btp_core.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a02fdb-9246-4993-9d64-0acde6a4d28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Custom MAPE function that ignores zero values in actuals (y_true)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    mask = y_true != 0  # Ignore zero values\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5a5dc3-e956-4169-922c-72f5031a1209",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First test without horizons\n",
    "# Step 1: Define the features and target variable\n",
    "all_features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM',  \n",
    "    'CONTRACT_DURATION_SUM', \n",
    "    'OVERCONSUMPTION_COUNT', \n",
    "    'ORDER_COUNT', \n",
    "    'ACTIVE_CONTRACT', \n",
    "    'TOTAL_CONSUMPTION_LAG_1'\n",
    "]\n",
    "\n",
    "X = df_aggregated_month_populated_btp_core[all_features].copy()\n",
    "y = df_aggregated_month_populated_btp_core['TOTAL_CONSUMPTION_SUM']\n",
    "\n",
    "# Step 2: Sort the data by date\n",
    "df_aggregated_month_populated_btp_core.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Step 3: Set up TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize an empty dictionary to store evaluation metrics\n",
    "feature_performance_tscv = {}\n",
    "\n",
    "# Loop to progressively remove features and evaluate performance\n",
    "while len(all_features) > 0:\n",
    "    print(f\"Using features: {all_features}\")\n",
    "    X_subset = X[all_features]\n",
    "    \n",
    "    # Initialize lists to store evaluation metrics for each split\n",
    "    mse_train_list, mse_test_list = [], []\n",
    "    r2_train_list, r2_test_list = [], []\n",
    "    mae_train_list, mae_test_list = [], []\n",
    "    mape_train_list, mape_test_list = [], []\n",
    "\n",
    "    # Perform time series cross-validation\n",
    "    for train_index, test_index in tscv.split(X_subset):\n",
    "        X_train, X_test = X_subset.iloc[train_index], X_subset.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train the Linear Regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "        mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "        mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        # Calculate MAPE while ignoring zero values\n",
    "        mape_train = np.mean(np.abs((y_train[y_train != 0] - y_train_pred[y_train != 0]) / y_train[y_train != 0])) * 100\n",
    "        mape_test = np.mean(np.abs((y_test[y_test != 0] - y_test_pred[y_test != 0]) / y_test[y_test != 0])) * 100\n",
    "        \n",
    "        # Store the metrics for each fold\n",
    "        mse_train_list.append(mse_train)\n",
    "        mse_test_list.append(mse_test)\n",
    "        r2_train_list.append(r2_train)\n",
    "        r2_test_list.append(r2_test)\n",
    "        mae_train_list.append(mae_train)\n",
    "        mae_test_list.append(mae_test)\n",
    "        mape_train_list.append(mape_train)\n",
    "        mape_test_list.append(mape_test)\n",
    "\n",
    "    # Calculate average metrics across all folds for this feature set\n",
    "    feature_performance_tscv[f'{len(all_features)}_features'] = {\n",
    "        'MSE_train': np.mean(mse_train_list),\n",
    "        'MSE_test': np.mean(mse_test_list),\n",
    "        'R²_train': np.mean(r2_train_list),\n",
    "        'R²_test': np.mean(r2_test_list),\n",
    "        'MAE_train': np.mean(mae_train_list),\n",
    "        'MAE_test': np.mean(mae_test_list),\n",
    "        'MAPE_train': np.mean(mape_train_list),\n",
    "        'MAPE_test': np.mean(mape_test_list)\n",
    "    }\n",
    "\n",
    "    # Determine the least significant feature to remove based on the MSE test performance\n",
    "    if len(all_features) > 1:\n",
    "        last_feature_performance = feature_performance_tscv[f'{len(all_features)}_features']\n",
    "        print(f\"Performance with {len(all_features)} features: MSE Test: {last_feature_performance['MSE_test']}\")\n",
    "        \n",
    "        # Fit the model with the current feature set to get coefficients\n",
    "        model.fit(X_subset, y)\n",
    "        coefficients = model.coef_\n",
    "        \n",
    "        # Create a DataFrame to store feature names and their coefficients\n",
    "        coeff_df = pd.DataFrame({\n",
    "            'Feature': all_features,\n",
    "            'Coefficient': coefficients\n",
    "        })\n",
    "        \n",
    "        # Print the coefficients\n",
    "        print(coeff_df)\n",
    "        \n",
    "        # Plot the coefficients\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(coeff_df['Feature'], coeff_df['Coefficient'], color='skyblue')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.title('Feature Coefficients')\n",
    "        plt.axvline(0, color='grey', linestyle='--')  # Vertical line at 0\n",
    "        plt.show()\n",
    "        \n",
    "        # Drop the least significant feature\n",
    "        all_features.remove(min(all_features, key=lambda f: last_feature_performance['MSE_test']))\n",
    "\n",
    "    else:  # Exit the loop if only one feature remains\n",
    "        print(f\"Only one feature left: {all_features[0]}. Stopping feature removal.\")\n",
    "        break\n",
    "\n",
    "# Step 5: Display performance for each feature set across all folds\n",
    "for feature_count, metrics in feature_performance_tscv.items():\n",
    "    print(f\"Results for {feature_count}:\")\n",
    "    print(f\"  MSE (Train): {metrics['MSE_train']}, MSE (Test): {metrics['MSE_test']}\")\n",
    "    print(f\"  R² (Train): {metrics['R²_train']}, R² (Test): {metrics['R²_test']}\")\n",
    "    print(f\"  MAE (Train): {metrics['MAE_train']}, MAE (Test): {metrics['MAE_test']}\")\n",
    "    print(f\"  MAPE (Train): {metrics['MAPE_train']}, MAPE (Test): {metrics['MAPE_test']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283126a2-7ee7-4c17-a7d5-32d7efbbe41f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Models for each horizon\n",
    "horizons = [1, 2, 3]\n",
    "results_overall = {}\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original\n",
    "df_horizon = df_aggregated_month_populated_btp_core.copy()\n",
    "\n",
    "# Define a name for the DataFrame for model saving\n",
    "df_name = \"df_aggregated_month_populated_btp_core\"\n",
    "\n",
    "all_features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM',  \n",
    "    'CONTRACT_DURATION_SUM', \n",
    "    'OVERCONSUMPTION_COUNT', \n",
    "    'ORDER_COUNT',  \n",
    "    'TOTAL_CONSUMPTION_LAG_1'  # Keep only one lag feature to avoid redundancy\n",
    "]\n",
    "\n",
    "# Process each horizon\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "\n",
    "    # Define the target variable for each horizon (shifted by -h)\n",
    "    df_horizon[f'TOTAL_CONSUMPTION_{h}'] = df_horizon['TOTAL_CONSUMPTION_SUM'].shift(-h)\n",
    "\n",
    "    # Only drop NaN values in the target variable, not the features\n",
    "    df_horizon_filtered = df_horizon.dropna(subset=[f'TOTAL_CONSUMPTION_{h}'])\n",
    "\n",
    "\n",
    "    # Define X (input features) to include all relevant features\n",
    "    X = df_horizon_filtered[all_features]\n",
    "    y = df_horizon_filtered[f'TOTAL_CONSUMPTION_{h}']\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits as necessary\n",
    "    \n",
    "    # Initialize metrics storage for cross-validation\n",
    "    metrics_per_fold = []\n",
    "\n",
    "    # Ensure sorting by DATE and CUSTOMER_ID to maintain temporal order\n",
    "    df_horizon_filtered.sort_values(by=['CUSTOMER_ID', 'DATE'], inplace=True)\n",
    "\n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Initialize Linear Regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        # Store metrics for the current fold\n",
    "        metrics_per_fold.append({\n",
    "            'MSE': mse,\n",
    "            'R²': r2,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape\n",
    "        })\n",
    "\n",
    "    # Average the metrics across folds\n",
    "    results_overall[h] = {\n",
    "        'MSE': np.mean([m['MSE'] for m in metrics_per_fold]),\n",
    "        'R²': np.mean([m['R²'] for m in metrics_per_fold]),\n",
    "        'MAE': np.mean([m['MAE'] for m in metrics_per_fold]),\n",
    "        'MAPE': np.mean([m['MAPE'] for m in metrics_per_fold])\n",
    "    }\n",
    "    \n",
    "    # Save the model for the current horizon, incorporating the DataFrame name\n",
    "    joblib.dump(model, f\"linear_regression_model_{df_name}_horizon_{h}.pkl\")\n",
    "    print(f\"Saved Linear Regression model for horizon {h} as 'linear_regression_model_{df_name}_horizon_{h}.pkl'.\")\n",
    "\n",
    "# Print overall results for each horizon\n",
    "for h, metrics in results_overall.items():\n",
    "    print(f\"\\nResults for horizon {h} month(s):\")\n",
    "    print(f\"Mean MSE: {metrics['MSE']:.4f}\")\n",
    "    print(f\"Mean R²: {metrics['R²']:.4f}\")\n",
    "    print(f\"Mean MAE: {metrics['MAE']:.4f}\")\n",
    "    print(f\"Mean MAPE: {metrics['MAPE']:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664139fb-2083-4e41-96a7-1cc7650b9a42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process each horizon and display feature importance\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "\n",
    "    # Define the target variable for each horizon (shifted by -h)\n",
    "    df_horizon[f'TOTAL_CONSUMPTION_{h}'] = df_horizon['TOTAL_CONSUMPTION_SUM'].shift(-h)\n",
    "\n",
    "    # Only drop NaN values in the target variable, not the features\n",
    "    df_horizon_filtered = df_horizon.dropna(subset=[f'TOTAL_CONSUMPTION_{h}'])\n",
    "\n",
    "    # Ensure there are no unintended losses of rows due to feature NaNs for horizon 1\n",
    "    if h == 1:\n",
    "        print(f\"Rows after dropping NaN for horizon {h}: {df_horizon_filtered.shape[0]}\")\n",
    "\n",
    "    # Define X (input features) to include all relevant features\n",
    "    X = df_horizon_filtered[all_features]\n",
    "    y = df_horizon_filtered[f'TOTAL_CONSUMPTION_{h}']\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits as necessary\n",
    "    \n",
    "    # Initialize metrics storage for cross-validation\n",
    "    metrics_per_fold = []\n",
    "\n",
    "    # Ensure sorting by DATE and CUSTOMER_ID to maintain temporal order\n",
    "    df_horizon_filtered.sort_values(by=['CUSTOMER_ID', 'DATE'], inplace=True)\n",
    "\n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Initialize Linear Regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        # Store metrics for the current fold\n",
    "        metrics_per_fold.append({\n",
    "            'MSE': mse,\n",
    "            'R²': r2,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape\n",
    "        })\n",
    "\n",
    "    # Average the metrics across folds\n",
    "    results_overall[h] = {\n",
    "        'MSE': np.mean([m['MSE'] for m in metrics_per_fold]),\n",
    "        'R²': np.mean([m['R²'] for m in metrics_per_fold]),\n",
    "        'MAE': np.mean([m['MAE'] for m in metrics_per_fold]),\n",
    "        'MAPE': np.mean([m['MAPE'] for m in metrics_per_fold])\n",
    "    }\n",
    "\n",
    "    # Extract feature importance (coefficients) after model is trained\n",
    "    feature_importance = pd.Series(model.coef_, index=all_features)\n",
    "\n",
    "    # Normalize to get relative importance\n",
    "    feature_importance_normalized = abs(feature_importance) / sum(abs(feature_importance))\n",
    "\n",
    "    # Display feature importance for this horizon\n",
    "    print(f\"\\nFeature importance for horizon {h} month(s):\")\n",
    "    print(feature_importance_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96aeff9f-9a1d-4c75-a6bc-9ad965676bb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Linear Regression for active contract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe28eeb9-59ab-488d-8ade-463ea4afeb37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/df_filtered_active_customers.parquet'\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "df_filtered_active_customers = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(df_filtered_active_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32004d7-8aba-42a6-98fb-d0a858869dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define the features and target variable\n",
    "features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM',  \n",
    "    'CONTRACT_DURATION_SUM', 'OVERCONSUMPTION_COUNT', 'ORDER_COUNT', \n",
    "    'ACTIVE_CONTRACT', 'TOTAL_CONSUMPTION_LAG_1'\n",
    "]\n",
    "\n",
    "X = df_filtered_active_customers[features].copy()  # Feature set\n",
    "y = df_filtered_active_customers['TOTAL_CONSUMPTION_SUM']  # Target variable\n",
    "\n",
    "# Step 2: Sort the DataFrame by DATE to maintain temporal order\n",
    "df_filtered_active_customers.sort_values(by='DATE', inplace=True)\n",
    "\n",
    "# Step 3: Apply target encoding to categorical columns\n",
    "categorical_columns = ['ISS_TEXT', 'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE']\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_columns)\n",
    "X_encoded = target_encoder.fit_transform(X, y)\n",
    "\n",
    "# Step 4: Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits based on your preference\n",
    "\n",
    "# Step 5: Train and evaluate a linear regression model using time series cross-validation\n",
    "model = LinearRegression()\n",
    "\n",
    "# Store metrics for each fold\n",
    "mse_train_list, r2_train_list, mae_train_list, mape_train_list = [], [], [], []\n",
    "mse_test_list, r2_test_list, mae_test_list, mape_test_list = [], [], [], []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_encoded):\n",
    "    # Create training and test sets\n",
    "    X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions for both train and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for the training set\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mape_train = mean_absolute_percentage_error(y_train.to_numpy(), y_train_pred)\n",
    "    \n",
    "    # Calculate metrics for the test set\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    mape_test = mean_absolute_percentage_error(y_test.to_numpy(), y_test_pred)\n",
    "    \n",
    "    # Append results for each fold\n",
    "    mse_train_list.append(mse_train)\n",
    "    r2_train_list.append(r2_train)\n",
    "    mae_train_list.append(mae_train)\n",
    "    mape_train_list.append(mape_train)\n",
    "    \n",
    "    mse_test_list.append(mse_test)\n",
    "    r2_test_list.append(r2_test)\n",
    "    mae_test_list.append(mae_test)\n",
    "    mape_test_list.append(mape_test)\n",
    "\n",
    "# Step 6: Calculate the average and standard deviation of the metrics across all folds\n",
    "average_metrics = {\n",
    "    'MSE Train': np.mean(mse_train_list),\n",
    "    'R² Train': np.mean(r2_train_list),\n",
    "    'MAE Train': np.mean(mae_train_list),\n",
    "    'MAPE Train': np.mean(mape_train_list),\n",
    "    'MSE Test': np.mean(mse_test_list),\n",
    "    'R² Test': np.mean(r2_test_list),\n",
    "    'MAE Test': np.mean(mae_test_list),\n",
    "    'MAPE Test': np.mean(mape_test_list)\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'MSE Train': np.std(mse_train_list),\n",
    "    'R² Train': np.std(r2_train_list),\n",
    "    'MAE Train': np.std(mae_train_list),\n",
    "    'MAPE Train': np.std(mape_train_list),\n",
    "    'MSE Test': np.std(mse_test_list),\n",
    "    'R² Test': np.std(r2_test_list),\n",
    "    'MAE Test': np.std(mae_test_list),\n",
    "    'MAPE Test': np.std(mape_test_list)\n",
    "}\n",
    "\n",
    "# Print the averaged results across all folds\n",
    "print(\"Averaged Results across all folds:\")\n",
    "for metric, value in average_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Optionally, print the standard deviation for a sense of variance across folds\n",
    "print(\"\\nStandard deviation across all folds:\")\n",
    "for metric, value in std_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7db991-2dc6-483e-8b87-1f3a3cd8e524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions and actual values for both train and test sets\n",
    "y_train_all, y_train_pred_all = [], []\n",
    "y_test_all, y_test_pred_all = [], []\n",
    "\n",
    "# Collect predictions for all folds\n",
    "for train_index, test_index in tscv.split(X_encoded):\n",
    "    # Create training and test sets\n",
    "    X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions for both train and test sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Append predictions and actual values\n",
    "    y_train_all.extend(y_train)\n",
    "    y_train_pred_all.extend(y_train_pred)\n",
    "    y_test_all.extend(y_test)\n",
    "    y_test_pred_all.extend(y_test_pred)\n",
    "\n",
    "# Convert lists to numpy arrays for easier manipulation\n",
    "y_train_all = np.array(y_train_all)\n",
    "y_train_pred_all = np.array(y_train_pred_all)\n",
    "y_test_all = np.array(y_test_all)\n",
    "y_test_pred_all = np.array(y_test_pred_all)\n",
    "\n",
    "# Step 1: Scatter Plot of Predicted vs Actual Values for Training Set\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=y_train_all, y=y_train_pred_all, color='blue', alpha=0.5)\n",
    "plt.plot([y_train_all.min(), y_train_all.max()], [y_train_all.min(), y_train_all.max()], 'r--')  # 45-degree line\n",
    "plt.title('Training Set: Predicted vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Step 2: Scatter Plot of Predicted vs Actual Values for Testing Set\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=y_test_all, y=y_test_pred_all, color='green', alpha=0.5)\n",
    "plt.plot([y_test_all.min(), y_test_all.max()], [y_test_all.min(), y_test_all.max()], 'r--')  # 45-degree line\n",
    "plt.title('Testing Set: Predicted vs Actual')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Residuals Plot for Testing Set\n",
    "residuals = y_test_all - y_test_pred_all\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test_pred_all, y=residuals, color='purple', alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac826781-7837-439b-82d5-65e754c79263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model for each horizon\n",
    "horizons = [1, 2, 3]\n",
    "results_overall = {}\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original\n",
    "df_horizon = df_filtered_active_customers.copy()\n",
    "\n",
    "# Define a name for the DataFrame for model saving\n",
    "df_name = \"df_filtered_active_customers\"\n",
    "\n",
    "all_features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM',  \n",
    "    'CONTRACT_DURATION_SUM', \n",
    "    'OVERCONSUMPTION_COUNT', \n",
    "    'ORDER_COUNT',  \n",
    "    'TOTAL_CONSUMPTION_LAG_1'  # Keep only one lag feature to avoid redundancy\n",
    "]\n",
    "\n",
    "# Process each horizon\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "\n",
    "    # Define the target variable for each horizon (shifted by -h)\n",
    "    df_horizon[f'TOTAL_CONSUMPTION_{h}'] = df_horizon['TOTAL_CONSUMPTION_SUM'].shift(-h)\n",
    "\n",
    "    # Only drop NaN values in the target variable, not the features\n",
    "    df_horizon_filtered = df_horizon.dropna(subset=[f'TOTAL_CONSUMPTION_{h}'])\n",
    "\n",
    "    # Ensure there are no unintended losses of rows due to feature NaNs for horizon 1\n",
    "    if h == 1:\n",
    "        print(f\"Rows after dropping NaN for horizon {h}: {df_horizon_filtered.shape[0]}\")\n",
    "\n",
    "    # Define X (input features) to include all relevant features\n",
    "    X = df_horizon_filtered[all_features]\n",
    "    y = df_horizon_filtered[f'TOTAL_CONSUMPTION_{h}']\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits as necessary\n",
    "    \n",
    "    # Initialize metrics storage for cross-validation\n",
    "    metrics_per_fold = []\n",
    "\n",
    "    # Ensure sorting by DATE and CUSTOMER_ID to maintain temporal order\n",
    "    df_horizon_filtered.sort_values(by=['CUSTOMER_ID', 'DATE'], inplace=True)\n",
    "\n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Initialize Linear Regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        # Store metrics for the current fold\n",
    "        metrics_per_fold.append({\n",
    "            'MSE': mse,\n",
    "            'R²': r2,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape\n",
    "        })\n",
    "\n",
    "    # Average the metrics across folds\n",
    "    results_overall[h] = {\n",
    "        'MSE': np.mean([m['MSE'] for m in metrics_per_fold]),\n",
    "        'R²': np.mean([m['R²'] for m in metrics_per_fold]),\n",
    "        'MAE': np.mean([m['MAE'] for m in metrics_per_fold]),\n",
    "        'MAPE': np.mean([m['MAPE'] for m in metrics_per_fold])\n",
    "    }\n",
    "    \n",
    "    # Save the model for the current horizon, incorporating the DataFrame name\n",
    "    joblib.dump(model, f\"linear_regression_model_{df_name}_horizon_{h}.pkl\")\n",
    "    print(f\"Saved Linear Regression model for horizon {h} as 'linear_regression_model_{df_name}_horizon_{h}.pkl'.\")\n",
    "\n",
    "# Print overall results for each horizon\n",
    "for h, metrics in results_overall.items():\n",
    "    print(f\"\\nResults for horizon {h} month(s):\")\n",
    "    print(f\"Mean MSE: {metrics['MSE']:.4f}\")\n",
    "    print(f\"Mean R²: {metrics['R²']:.4f}\")\n",
    "    print(f\"Mean MAE: {metrics['MAE']:.4f}\")\n",
    "    print(f\"Mean MAPE: {metrics['MAPE']:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95874e37-ee61-4b01-a2af-06a82ff48e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process each horizon and display feature importance\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "\n",
    "    # Define the target variable for each horizon (shifted by -h)\n",
    "    df_horizon[f'TOTAL_CONSUMPTION_{h}'] = df_horizon['TOTAL_CONSUMPTION_SUM'].shift(-h)\n",
    "\n",
    "    # Only drop NaN values in the target variable, not the features\n",
    "    df_horizon_filtered = df_horizon.dropna(subset=[f'TOTAL_CONSUMPTION_{h}'])\n",
    "\n",
    "\n",
    "    # Define X (input features) to include all relevant features\n",
    "    X = df_horizon_filtered[all_features]\n",
    "    y = df_horizon_filtered[f'TOTAL_CONSUMPTION_{h}']\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)  # Adjust n_splits as necessary\n",
    "    \n",
    "    # Initialize metrics storage for cross-validation\n",
    "    metrics_per_fold = []\n",
    "\n",
    "    # Ensure sorting by DATE and CUSTOMER_ID to maintain temporal order\n",
    "    df_horizon_filtered.sort_values(by=['CUSTOMER_ID', 'DATE'], inplace=True)\n",
    "\n",
    "    # Perform TimeSeriesSplit cross-validation\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Initialize Linear Regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        # Store metrics for the current fold\n",
    "        metrics_per_fold.append({\n",
    "            'MSE': mse,\n",
    "            'R²': r2,\n",
    "            'MAE': mae,\n",
    "            'MAPE': mape\n",
    "        })\n",
    "\n",
    "    # Average the metrics across folds\n",
    "    results_overall[h] = {\n",
    "        'MSE': np.mean([m['MSE'] for m in metrics_per_fold]),\n",
    "        'R²': np.mean([m['R²'] for m in metrics_per_fold]),\n",
    "        'MAE': np.mean([m['MAE'] for m in metrics_per_fold]),\n",
    "        'MAPE': np.mean([m['MAPE'] for m in metrics_per_fold])\n",
    "    }\n",
    "\n",
    "    # Extract feature importance (coefficients) after model is trained\n",
    "    feature_importance = pd.Series(model.coef_, index=all_features)\n",
    "\n",
    "    # Normalize to get relative importance\n",
    "    feature_importance_normalized = abs(feature_importance) / sum(abs(feature_importance))\n",
    "\n",
    "    # Display feature importance for this horizon\n",
    "    print(f\"\\nFeature importance for horizon {h} month(s):\")\n",
    "    print(feature_importance_normalized)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LINEAR_REGRESSION_NEW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
