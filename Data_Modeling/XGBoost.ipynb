{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ff7e73-7223-478a-a26e-a46c6e44929e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e074eef-855c-4f25-8ef5-203914ab50e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_1_path = '../Datasets/df_aggregated_month_populated_btp_core.parquet'\n",
    "dataset_2_path = '../Datasets/df_filtered_active_customers.parquet'\n",
    "df_aggregated = pd.read_parquet(dataset_1_path, engine='pyarrow')\n",
    "df_filtered = pd.read_parquet(dataset_2_path, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb922a29-bfd5-49de-9c78-845a34a5fcb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def custom_mape(y_true, y_pred):\n",
    "    # Calculate MAPE while ignoring zeros in y_true\n",
    "    non_zero_indices = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[non_zero_indices] - y_pred[non_zero_indices]) / y_true[non_zero_indices])) * 100\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a9c464-fe21-4d62-8e1a-d77587947f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Objective function for hyperparameter tuning\n",
    "def objective(params, X_train, y_train):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', enable_categorical=True, random_state=42, **params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    mean_rmse = np.sqrt(-cv_scores.mean())\n",
    "    return {'loss': mean_rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2648027e-8308-4895-9928-adc8cd9020e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Map indices to actual hyperparameter values after optimization\n",
    "def map_index_to_value(best_params):\n",
    "    best_params['n_estimators'] = [50, 100, 200][best_params['n_estimators']]\n",
    "    best_params['learning_rate'] = [0.001, 0.05, 0.1][best_params['learning_rate']]\n",
    "    best_params['max_depth'] = [3, 5, 6][best_params['max_depth']]\n",
    "    best_params['min_child_weight'] = [1, 3, 5][best_params['min_child_weight']]\n",
    "    best_params['gamma'] = [0, 0.5, 1][best_params['gamma']]\n",
    "    best_params['subsample'] = [0.6, 0.8, 1.0][best_params['subsample']]\n",
    "    best_params['colsample_bytree'] = [0.6, 0.8, 1.0][best_params['colsample_bytree']]\n",
    "    best_params['reg_alpha'] = [0, 0.5, 1.0][best_params['reg_alpha']]\n",
    "    best_params['reg_lambda'] = [1.0, 10.0, 20.0][best_params['reg_lambda']]\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model\n",
    "def evaluate_model(X_train, y_train, best_params):\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', enable_categorical=True, random_state=42, **best_params)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation and return metrics\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    y_pred = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Calculate the metrics\n",
    "    y_pred_mean = -y_pred.mean()\n",
    "    rmse = np.sqrt(y_pred_mean)\n",
    "    r2 = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2').mean()\n",
    "    mae = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_absolute_error').mean()\n",
    "    mape = custom_mape(y_train, model.predict(X_train))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and target variable setup\n",
    "features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM', 'LICENSE_COUNT_SUM', 'LATEST_CONTRACT_MIN', \n",
    "    'CONTRACT_DURATION_SUM', 'CONTRACT_DURATION_MEAN', 'OVERCONSUMPTION_COUNT', \n",
    "    'ORDER_COUNT', 'BUNDLE_INDICATOR', 'INTEGRATION_SUITE', 'CLOUD_INTEGRATION',\n",
    "    'ACTIVE_CONTRACT', 'TOTAL_CONSUMPTION_LAG_1', 'TOTAL_CONSUMPTION_LAG_2', \n",
    "    'TOTAL_CONSUMPTION_LAG_3', 'TOTAL_CONSUMPTION_ROLLING_3', 'TOTAL_CONSUMPTION_ROLLING_6', \n",
    "    'MONTH', 'MONTH_SIN', 'MONTH_COS', 'TREND', 'TOTAL_CONSUMPTION_DIFF_1', \n",
    "    'TOTAL_CONSUMPTION_CUMSUM', 'TOTAL_CONSUMPTION_EMA_3', 'YEAR', \n",
    "    'ISS_TEXT', 'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE'\n",
    "]\n",
    "X_base = df_aggregated[features].copy()\n",
    "categorical_columns = ['ISS_TEXT', 'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE']\n",
    "X_base[categorical_columns] = X_base[categorical_columns].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879b054f-5f39-4977-9968-60b011fe9f30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forecasting and hyperparameter tuning loop\n",
    "horizons = [1, 2, 3]\n",
    "results_overall = {}\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "    \n",
    "    # Align X and y for each horizon\n",
    "    y = df_aggregated['TOTAL_CONSUMPTION_SUM'].shift(-h).dropna()\n",
    "    X = X_base.iloc[:-h].copy()\n",
    "    \n",
    "    # Hyperparameter search space\n",
    "    space = {\n",
    "        'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "        'learning_rate': hp.choice('learning_rate', [0.001, 0.05, 0.1]),\n",
    "        'max_depth': hp.choice('max_depth', [3, 5, 6]),\n",
    "        'min_child_weight': hp.choice('min_child_weight', [1, 3, 5]),\n",
    "        'gamma': hp.choice('gamma', [0, 0.5, 1]),\n",
    "        'subsample': hp.choice('subsample', [0.6, 0.8, 1.0]),\n",
    "        'colsample_bytree': hp.choice('colsample_bytree', [0.6, 0.8, 1.0]),\n",
    "        'reg_alpha': hp.choice('reg_alpha', [0, 0.5, 1.0]),\n",
    "        'reg_lambda': hp.choice('reg_lambda', [1.0, 10.0, 20.0])\n",
    "    }\n",
    "    \n",
    "    # Hyperparameter tuning with fmin\n",
    "    print(f\"\\nRunning hyperparameter tuning for horizon {h} month(s)\")\n",
    "    best_params = fmin(fn=lambda params: objective(params, X, y), space=space, algo=tpe.suggest, max_evals=100, trials=spark_trials)\n",
    "    best_params_mapped = map_index_to_value(best_params)\n",
    "    \n",
    "    # Evaluate model with best parameters\n",
    "    metrics = evaluate_model(X, y, best_params_mapped)\n",
    "    results_overall[h] = metrics  # Store results for each horizon\n",
    "\n",
    "# Output results\n",
    "for h, metrics in results_overall.items():\n",
    "    print(f\"\\nResults for horizon {h} month(s):\")\n",
    "    print(f\"Mean RMSE: {metrics['RMSE']}\")\n",
    "    print(f\"Mean R²: {metrics['R²']}\")\n",
    "    print(f\"Mean MAE: {metrics['MAE']}\")\n",
    "    print(f\"Mean MAPE: {metrics['MAPE']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea94d2b-f5e8-44a4-a37d-a06f21fbaa92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "XGboost for active contract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c363b5a-49bf-402d-ab68-664d14197f4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature and target variable setup\n",
    "features = [\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_SUM', 'LICENSE_COUNT_SUM', 'LATEST_CONTRACT_MIN', \n",
    "    'CONTRACT_DURATION_SUM', 'CONTRACT_DURATION_MEAN', 'OVERCONSUMPTION_COUNT', \n",
    "    'ORDER_COUNT', 'BUNDLE_INDICATOR', 'INTEGRATION_SUITE', 'CLOUD_INTEGRATION',\n",
    "    'TOTAL_CONSUMPTION_LAG_1', 'TOTAL_CONSUMPTION_LAG_2', \n",
    "    'TOTAL_CONSUMPTION_LAG_3', 'TOTAL_CONSUMPTION_ROLLING_3', 'TOTAL_CONSUMPTION_ROLLING_6', \n",
    "    'MONTH', 'MONTH_SIN', 'MONTH_COS', 'TREND', 'TOTAL_CONSUMPTION_DIFF_1', \n",
    "    'TOTAL_CONSUMPTION_CUMSUM', 'TOTAL_CONSUMPTION_EMA_3', 'YEAR', \n",
    "    'ISS_TEXT', 'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE'\n",
    "]\n",
    "X_base = df_filtered[features].copy()\n",
    "categorical_columns = ['ISS_TEXT', 'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE']\n",
    "X_base[categorical_columns] = X_base[categorical_columns].astype('category')\n",
    "# Forecasting and hyperparameter tuning loop\n",
    "horizons = [1, 2, 3]\n",
    "results_overall = {}\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\nProcessing horizon: {h} month(s) ahead\")\n",
    "    \n",
    "    # Align X and y for each horizon\n",
    "    y = df_filtered['TOTAL_CONSUMPTION_SUM'].shift(-h).dropna()\n",
    "    X = X_base.iloc[:-h].copy()\n",
    "    \n",
    "    # Hyperparameter search space\n",
    "    space = {\n",
    "        'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "        'learning_rate': hp.choice('learning_rate', [0.001, 0.05, 0.1]),\n",
    "        'max_depth': hp.choice('max_depth', [3, 5, 6]),\n",
    "        'min_child_weight': hp.choice('min_child_weight', [1, 3, 5]),\n",
    "        'gamma': hp.choice('gamma', [0, 0.5, 1]),\n",
    "        'subsample': hp.choice('subsample', [0.6, 0.8, 1.0]),\n",
    "        'colsample_bytree': hp.choice('colsample_bytree', [0.6, 0.8, 1.0]),\n",
    "        'reg_alpha': hp.choice('reg_alpha', [0, 0.5, 1.0]),\n",
    "        'reg_lambda': hp.choice('reg_lambda', [1.0, 10.0, 20.0])\n",
    "    }\n",
    "    \n",
    "    # Hyperparameter tuning with fmin\n",
    "    print(f\"\\nRunning hyperparameter tuning for horizon {h} month(s)\")\n",
    "    best_params = fmin(fn=lambda params: objective(params, X, y), space=space, algo=tpe.suggest, max_evals=100, trials=spark_trials)\n",
    "    best_params_mapped = map_index_to_value(best_params)\n",
    "    \n",
    "    # Evaluate model with best parameters\n",
    "    metrics = evaluate_model(X, y, best_params_mapped)\n",
    "    results_overall[h] = metrics  # Store results for each horizon\n",
    "\n",
    "# Output results\n",
    "for h, metrics in results_overall.items():\n",
    "    print(f\"\\nResults for horizon {h} month(s):\")\n",
    "    print(f\"Mean RMSE: {metrics['RMSE']}\")\n",
    "    print(f\"Mean R²: {metrics['R²']}\")\n",
    "    print(f\"Mean MAE: {metrics['MAE']}\")\n",
    "    print(f\"Mean MAPE: {metrics['MAPE']}%\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "XG_BOOST_NEW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
