{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e060c1-6047-42f5-9b6b-d0fcf001ccd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71abe3a1-1fd6-4961-b7c3-1a600f7e7c85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get Raw BTP Core Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b49ee27-7289-4ffb-b155-81308513461f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#credentials\n",
    "\n",
    "scope = \"xxxxxxx\"\n",
    "username = \"xxxxxxx\"\n",
    "password_key = \"xxxxx\"\n",
    "password = dbutils.secrets.get(scope=scope, key=password_key)\n",
    "\n",
    "\n",
    "jdbc_url = \"xxx\"\n",
    "jdbc_driver = \"xxx\"\n",
    "jdbc_properties = {\n",
    "    \"user\" : username,\n",
    "    \"password\" : password,\n",
    "    \"driver\" : jdbc_driver,\n",
    "    \"encrypt\": \"true\",\n",
    "    \"validateCertificate\": \"false\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5cf59c-3262-41b5-8d71-8413829db5aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get data from BWP\n",
    "\n",
    "def spark_read_hana(query):\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", jdbc_driver) \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .option(\"encrypt\", True) \\\n",
    "        .option(\"validateCertificate\", False) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    df.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacb6215-130d-47ab-b6bc-7eccff0d9b91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cacv_query = 'SELECT \"MONTH\", \"CUSTOMER_ID\", \"IS_BUNDLE\", \"SOURCE\", \"ORDER_ID\", \"IS_OVERCONSUMPTION\", \"CONTRACT_START_DATE\", \"CONTRACT_END_DATE\", \"TENANT_EXTERNAL_ID\", \"ACCOUNT_ID\", \"MATERIAL\", \"METRIC_ID\", \"PLANNING_ENTITY_ID\", \"PLANNING_ENTITY_NAME\", \"SERVICE_SHORT_NAME\", sum(\"TOTAL_CONSUMPTION\") AS \"TOTAL_CONSUMPTION\", sum(\"TOTAL_USAGE\") AS \"TOTAL_USAGE\", avg(\"CONSUMPTION_RATIO\") AS \"CONSUMPTION_RATIO\", sum(\"TOTAL_CONSUMPTION_KEUR\") AS \"TOTAL_CONSUMPTION_KEUR\", sum(\"MONTHLY_CONTRACT_NET_VALUE\") AS \"MONTHLY_CONTRACT_NET_VALUE\", sum(\"OVERAGE_CONSUMPTION\") AS \"OVERAGE_CONSUMPTION\", sum(\"TRAFFIC\") AS \"TRAFFIC\", sum(\"LICENSE_COUNT\") AS \"LICENSE_COUNT\" FROM \"_SYS_BIC\".\"corp.clus.cacv.c4s/CL_CORP_CLUS_CACV_C4S_CONSUMED_ACV_UNITED\" WHERE ( UPPER(\"MONTH\") NOT LIKE  UPPER(\\'%202408%\\')) AND \"SOURCE\" LIKE \\'BTP_CORE\\' GROUP BY \"MONTH\", \"CUSTOMER_ID\", \"IS_BUNDLE\", \"IS_OVERCONSUMPTION\", \"SOURCE\",\"ORDER_ID\", \"CONTRACT_START_DATE\", \"CONTRACT_END_DATE\", \"TENANT_EXTERNAL_ID\", \"ACCOUNT_ID\", \"MATERIAL\", \"METRIC_ID\",\"PLANNING_ENTITY_ID\", \"PLANNING_ENTITY_NAME\", \"SERVICE_SHORT_NAME\"'\n",
    "\n",
    "cacv_raw_df = spark_read_hana(cacv_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4f4f1e-f185-4684-8a8a-e1053717997f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert spark df to pandas df\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Convert DecimalType columns to DoubleType\n",
    "cacv_raw_df = cacv_raw_df.withColumn('TOTAL_CONSUMPTION', col('TOTAL_CONSUMPTION').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('TOTAL_USAGE', col('TOTAL_USAGE').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('CONSUMPTION_RATIO', col('CONSUMPTION_RATIO').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('MONTHLY_CONTRACT_NET_VALUE', col('MONTHLY_CONTRACT_NET_VALUE').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('TOTAL_CONSUMPTION_KEUR', col('TOTAL_CONSUMPTION_KEUR').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('TRAFFIC', col('TRAFFIC').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('LICENSE_COUNT', col('LICENSE_COUNT').cast(DoubleType()))\n",
    "cacv_raw_df = cacv_raw_df.withColumn('OVERAGE_CONSUMPTION', col('OVERAGE_CONSUMPTION').cast(DoubleType()))\n",
    "\n",
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "cacv_raw_pd_df = cacv_raw_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b954692-564a-4882-97f3-b14979123d69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_specific_columns_to_float(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            try:\n",
    "                df[column] = df[column].astype(float)\n",
    "                print(f\"Column '{column}' converted to float.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column '{column}' cannot be converted to float. Non-numeric data found.\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in the dataframe.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ad1fec-7d9a-4938-8877-b5092405f0d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to convert to float\n",
    "columns_to_convert = ['TOTAL_CONSUMPTION','TOTAL_USAGE','CONSUMPTION_RATIO','TOTAL_CONSUMPTION_KEUR', 'MONTHLY_CONTRACT_NET_VALUE', 'TRAFFIC', 'LICENSE_COUNT', 'OVERAGE_CONSUMPTION']\n",
    "\n",
    "cacv_raw_pd_df = convert_specific_columns_to_float(cacv_raw_pd_df, columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a725e5-28f2-4dd8-9ca9-a89ff493e7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/btp_core_raw.parquet'\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "cacv_raw_pd_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "cacv_raw_pd_df = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(cacv_raw_pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb0c0a19-9367-4e71-952f-f9644a7b3890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "BTP Core Data Feature Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c345f7-fdc1-43b2-962d-92d3ead6e652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Convert 'MONTH' from YYYYMM to YYYY-MM-01 format and then to datetime\n",
    "cacv_raw_pd_df['MONTH'] = cacv_raw_pd_df['MONTH'].apply(lambda x: pd.to_datetime(str(x) + '01', format='%Y%m%d', errors='coerce'))\n",
    "\n",
    "# Step 2: Convert 'CONTRACT_START_DATE' and 'CONTRACT_END_DATE' to datetime as well\n",
    "cacv_raw_pd_df['CONTRACT_START_DATE'] = pd.to_datetime(cacv_raw_pd_df['CONTRACT_START_DATE'], errors='coerce')\n",
    "cacv_raw_pd_df['CONTRACT_END_DATE'] = pd.to_datetime(cacv_raw_pd_df['CONTRACT_END_DATE'], errors='coerce')\n",
    "\n",
    "# Step 3: Calculate LATEST_CONTRACT as the difference in months between 'MONTH' and 'CONTRACT_START_DATE'\n",
    "cacv_raw_pd_df['LATEST_CONTRACT'] = ((cacv_raw_pd_df['MONTH'].dt.year - cacv_raw_pd_df['CONTRACT_START_DATE'].dt.year) * 12 + \n",
    "                                          (cacv_raw_pd_df['MONTH'].dt.month - cacv_raw_pd_df['CONTRACT_START_DATE'].dt.month)).astype('Int64')\n",
    "\n",
    "# Step 4: Calculate the contract duration in months between 'CONTRACT_START_DATE' and 'CONTRACT_END_DATE'\n",
    "cacv_raw_pd_df['CONTRACT_DURATION'] = ((cacv_raw_pd_df['CONTRACT_END_DATE'].dt.year - cacv_raw_pd_df['CONTRACT_START_DATE'].dt.year) * 12 + \n",
    "                                            (cacv_raw_pd_df['CONTRACT_END_DATE'].dt.month - cacv_raw_pd_df['CONTRACT_START_DATE'].dt.month)).astype('Int64')\n",
    "\n",
    "# Step 5: Filter the DataFrame to keep only the specified columns plus the new calculated ones\n",
    "columns_to_keep = ['MONTH', 'CUSTOMER_ID', 'TOTAL_CONSUMPTION', 'TOTAL_USAGE', 'CONSUMPTION_RATIO', \n",
    "                   'MONTHLY_CONTRACT_NET_VALUE', 'OVERAGE_CONSUMPTION', 'TRAFFIC', 'LICENSE_COUNT', \n",
    "                   'ORDER_ID', 'LATEST_CONTRACT', 'CONTRACT_DURATION', 'IS_OVERCONSUMPTION', 'IS_BUNDLE', 'SERVICE_SHORT_NAME']\n",
    "\n",
    "filtered_df = cacv_raw_pd_df[columns_to_keep]\n",
    "\n",
    "# Step 6: Continue with further analysis or aggregation\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50213561-4f5c-4e8b-a78d-3c96ed7e18da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/btp_core_raw_feature_enhanced.parquet'\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "filtered_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "btp_core_raw_feature_enhanced = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(btp_core_raw_feature_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d29f51-ef43-4fb6-bebd-73cc89c452db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "BTP Core Data Aggregated on Month/Customer and Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25275cd-d561-4fde-84b7-1e9eefb995e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create Integration Suite and Cloud Integration columns based on SERVICE_SHORT_NAME\n",
    "btp_core_raw_feature_enhanced['Integration_Suite'] = np.where(\n",
    "    btp_core_raw_feature_enhanced['SERVICE_SHORT_NAME'].str.contains('Integration Suite', case=False), 1, 0)\n",
    "\n",
    "btp_core_raw_feature_enhanced['Cloud_Integration'] = np.where(\n",
    "    btp_core_raw_feature_enhanced['SERVICE_SHORT_NAME'].str.contains('Cloud Integration', case=False), 1, 0)\n",
    "\n",
    "# Step 2: Group by MONTH and CUSTOMER_ID, aggregate, and set 1 or 0 for Integration Suite and Cloud Integration\n",
    "aggregated_df = btp_core_raw_feature_enhanced.groupby(['MONTH', 'CUSTOMER_ID']).agg({\n",
    "    'TOTAL_CONSUMPTION': 'sum',\n",
    "    'TOTAL_USAGE': 'sum',\n",
    "    'CONSUMPTION_RATIO': 'mean',\n",
    "    'OVERAGE_CONSUMPTION': 'sum',\n",
    "    'TRAFFIC': 'sum',\n",
    "    'LICENSE_COUNT': 'sum',\n",
    "    'MONTHLY_CONTRACT_NET_VALUE': 'sum',\n",
    "    'LATEST_CONTRACT': 'min',  # Get the latest contract start date\n",
    "    'CONTRACT_DURATION': ['sum', 'mean'],  # Get both the sum and the mean of contract durations\n",
    "    'IS_OVERCONSUMPTION': lambda x: (x == 'Y').sum(),  # Count how many orders are overconsuming\n",
    "    'IS_BUNDLE': lambda x: 1 if 'Y' in x.values else 0,  # Binary value for IS_BUNDLE\n",
    "    'ORDER_ID': 'nunique',  # Count the number of distinct orders\n",
    "    'Integration_Suite': 'max',  # Aggregate Integration Suite column\n",
    "    'Cloud_Integration': 'max'   # Aggregate Cloud Integration column\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Rename the columns to more meaningful names\n",
    "aggregated_df.columns = ['_'.join(col).strip('_') for col in aggregated_df.columns.values]\n",
    "\n",
    "# Step 4: Rename specific columns\n",
    "aggregated_df.rename(columns={\n",
    "    'ORDER_ID_nunique': 'ORDER_COUNT',\n",
    "    'IS_OVERCONSUMPTION_<lambda>': 'OVERCONSUMPTION_COUNT',\n",
    "    'IS_BUNDLE_<lambda>': 'BUNDLE_INDICATOR',\n",
    "    'CONTRACT_DURATION_sum': 'CONTRACT_DURATION_SUM',\n",
    "    'CONTRACT_DURATION_mean': 'CONTRACT_DURATION_MEAN',\n",
    "    'Integration_Suite_max': 'Integration_Suite',\n",
    "    'Cloud_Integration_max': 'Cloud_Integration'\n",
    "}, inplace=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103142a4-37c1-415e-8729-d797a5df1f66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new active Contract Field\n",
    "aggregated_df['Active_Contract'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551e3b2d-2ce4-48e9-a85a-92c1361ea93e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the MONTH column is in datetime format\n",
    "aggregated_df['MONTH'] = pd.to_datetime(aggregated_df['MONTH'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caec67bb-d6a3-4974-9614-a473d1abbe4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/aggregated_btp_core.parquet'\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "aggregated_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "aggregated_btp_core = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(aggregated_btp_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e8eaa7-2eb1-4349-974d-295ab8c75c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Get Raw Account Main Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd53ef2c-b20a-4613-88a8-530f619feb9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get MADA data\n",
    "mada_query = 'SELECT * FROM \"_SYS_BIC\".\"corp.clus.crossCacv.core/CL_CORP_CLUS_CROSS_CACV_MADA/dp/ACCOUNT_MAIN\"'\n",
    "\n",
    "mada_df = spark_read_hana(mada_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85447e9-5643-433d-9bac-100358a76d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert PySpark DataFrame to pandas DataFrame\n",
    "mada_pd_df = mada_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dd7e9a9-2f10-4787-999c-136a68079c83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/Mada.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507e9d3a-4b2f-47d3-ba6a-f23b6c9ffd2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save DataFrame to Parquet\n",
    "mada_pd_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4bc09d-7fc7-42ef-af31-218b8565db2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/Mada.parquet'\n",
    "# Read DataFrame from Parquet\n",
    "Mada = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(Mada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3b9449-55bd-4e89-a39c-c7b57705a3ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Join Mada Account Main Data with BTP Core Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a92264-d255-4712-9960-79084e6ccc20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform an inner join on PBUP_AC and CUSTOMER_ID\n",
    "merged_df = pd.merge(aggregated_btp_core, Mada, left_on='CUSTOMER_ID', right_on='PBUP_AC', how='left')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65201b1f-a5d9-47f3-8c89-9964afccebc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the columns to be dropped\n",
    "columns_to_drop = [\n",
    "    'PBUP_AC', \n",
    "    'CUSTOMER_TEXT', \n",
    "    'ISS_ID', \n",
    "    'PLANNING_ENTITY_ID', \n",
    "    'PLANNING_ENTITY_TEXT',\n",
    "    'TOTAL_USAGE_sum', \n",
    "    'CONSUMPTION_RATIO_mean', \n",
    "    'OVERAGE_CONSUMPTION_sum', \n",
    "    'TRAFFIC_sum'\n",
    "]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "filtered_merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop rows where CUSTOMER_ID is '0018894795', '0050124806', or '0050145084'\n",
    "filtered_merged_df = filtered_merged_df[~filtered_merged_df['CUSTOMER_ID'].isin(['0018894795', '0050124806', '0050145084'])]\n",
    "\n",
    "# Rename specified columns\n",
    "filtered_merged_df = filtered_merged_df.rename(columns={\n",
    "    'MONTH': 'DATE',\n",
    "    'MONTHLY_CONTRACT_NET_VALUE_sum': 'MONTHLY_CONTRACT_NET_VALUE_SUM',\n",
    "    'LICENSE_COUNT_sum': 'LICENSE_COUNT_SUM',\n",
    "    'LATEST_CONTRACT_min': 'LATEST_CONTRACT_MIN',\n",
    "    'Integration_Suite': 'INTEGRATION_SUITE',\n",
    "    'Cloud_Integration': 'CLOUD_INTEGRATION',\n",
    "    'Active_Contract': 'ACTIVE_CONTRACT',\n",
    "    'TOTAL_CONSUMPTION_sum' : 'TOTAL_CONSUMPTION_SUM'\n",
    "})\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(filtered_merged_df)\n",
    "\n",
    "# Optionally, print the remaining column names\n",
    "print(filtered_merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f856cbac-5f2a-43ba-b11f-b61dd9ac87db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/df_aggregated_btp_core_with_mada.parquet'\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "filtered_merged_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3847078d-1c05-48a0-aa59-af67c922ba15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dataframe with populated months filled with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab982fc9-61a0-43e2-9dec-aa89fb10c961",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/df_aggregated_btp_core_with_mada.parquet'\n",
    "\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "filtered_merged_df = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(filtered_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db18eacf-3336-42e2-8a05-5ffb3bd5062e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert DATE to datetime if it's not already\n",
    "filtered_merged_df['DATE'] = pd.to_datetime(filtered_merged_df['DATE'])\n",
    "\n",
    "# Get the range of DATEs in the dataset\n",
    "all_DATEs = pd.date_range(start=filtered_merged_df['DATE'].min(), end=filtered_merged_df['DATE'].max(), freq='MS')\n",
    "\n",
    "# Create a complete DataFrame with all CUSTOMER_ID and DATE combinations\n",
    "customer_DATE_combinations = pd.MultiIndex.from_product(\n",
    "    [filtered_merged_df['CUSTOMER_ID'].unique(), all_DATEs], names=['CUSTOMER_ID', 'DATE'])\n",
    "\n",
    "complete_df = pd.DataFrame(index=customer_DATE_combinations).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the original data to fill in missing DATEs with NaN\n",
    "complete_df = complete_df.merge(filtered_merged_df, on=['CUSTOMER_ID', 'DATE'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1942f40-5614-4093-9da5-de8435427c57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to be filled with stable values\n",
    "stable_columns = [\n",
    "    'CUSTOMER_ID', 'ISS_TEXT', \n",
    "    'GLOBAL_REGION', 'COUNTRY', 'SAP_MASTER_CODE'\n",
    "]\n",
    "\n",
    "# Group by CUSTOMER_ID and apply forward fill followed by backward fill on the stable columns\n",
    "complete_df[stable_columns] = complete_df.groupby('CUSTOMER_ID')[stable_columns].ffill().bfill()\n",
    "\n",
    "\n",
    "# Print the resulting DataFrame to verify the filling\n",
    "complete_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df33ebb-52a1-46f2-a828-de735f079593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill with 0\n",
    "complete_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899c22fe-208e-4d2c-aa49-051fbf57682d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the DataFrame is sorted by CUSTOMER_ID and DATE in ascending order\n",
    "complete_df = complete_df.sort_values(by=['CUSTOMER_ID', 'DATE'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "# Create lag features for TOTAL_CONSUMPTION\n",
    "for lag in range(1, 4):  # Lag features for the last 3 months\n",
    "    complete_df[f'TOTAL_CONSUMPTION_LAG_{lag}'] = complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM'].shift(lag)\n",
    "\n",
    "# Create rolling averages (3-month and 6-month) and ensure proper index alignment\n",
    "complete_df['TOTAL_CONSUMPTION_ROLLING_3'] = (\n",
    "    complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM']\n",
    "    .rolling(window=3)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index to align with original DataFrame\n",
    ")\n",
    "\n",
    "complete_df['TOTAL_CONSUMPTION_ROLLING_6'] = (\n",
    "    complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM']\n",
    "    .rolling(window=6)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)  # Reset index to align with original DataFrame\n",
    ")\n",
    "\n",
    "# Create sine and cosine of month for seasonality\n",
    "complete_df['MONTH'] = complete_df['DATE'].dt.month  # Extract the month\n",
    "complete_df['MONTH_SIN'] = np.sin(2 * np.pi * complete_df['MONTH'] / 12)\n",
    "complete_df['MONTH_COS'] = np.cos(2 * np.pi * complete_df['MONTH'] / 12)\n",
    "\n",
    "# Create a trend feature (linear progression of months)\n",
    "complete_df['TREND'] = complete_df.groupby('CUSTOMER_ID').cumcount() + 1  # Cumulative count of months per customer\n",
    "\n",
    "# Create difference features (change in consumption from the previous month)\n",
    "complete_df['TOTAL_CONSUMPTION_DIFF_1'] = complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM'].diff(1)\n",
    "\n",
    "# Create cumulative sum of consumption (ensure proper index alignment)\n",
    "complete_df['TOTAL_CONSUMPTION_CUMSUM'] = (\n",
    "    complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM']\n",
    "    .cumsum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Create Exponential Moving Average (3-month EMA)\n",
    "complete_df['TOTAL_CONSUMPTION_EMA_3'] = (\n",
    "    complete_df.groupby('CUSTOMER_ID')['TOTAL_CONSUMPTION_SUM']\n",
    "    .ewm(span=3, adjust=False)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Create other features like YEAR if needed\n",
    "complete_df['YEAR'] = complete_df['DATE'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8112affd-a9c6-461c-8b6e-69c503b8ebe6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill with 0\n",
    "complete_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ebed58-33c2-48d9-8d91-6b419113a9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative path to the Dataset folder\n",
    "dataset_path = '../Datasets/df_aggregated_month_populated_btp_core.parquet'\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "complete_df.to_parquet(dataset_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Read DataFrame from Parquet\n",
    "df_aggregated_month_populated_btp_core = pd.read_parquet(dataset_path, engine='pyarrow')\n",
    "print(df_aggregated_month_populated_btp_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452644c2-f801-44ff-9af4-6488bc1193a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Subset of Dataframe with customers having an active contract for whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400dcbca-429e-4fa6-9caf-e06e5a32c913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by customer and count active contracts for each customer\n",
    "customer_active_counts = df_aggregated_month_populated_btp_core.groupby('CUSTOMER_ID')['ACTIVE_CONTRACT'].sum()\n",
    "\n",
    "# Filter customers who have 31 active months\n",
    "customers_with_full_active_contracts = customer_active_counts[customer_active_counts == 31].index\n",
    "\n",
    "# Filter the original DataFrame to keep only those customers\n",
    "df_filtered_active_customers = df_aggregated_month_populated_btp_core[df_aggregated_month_populated_btp_core['CUSTOMER_ID'].isin(customers_with_full_active_contracts)]\n",
    "\n",
    "# Save the filtered DataFrame to a new Parquet file\n",
    "filtered_dataset_path = '../Datasets/df_filtered_active_customers.parquet'\n",
    "df_filtered_active_customers.to_parquet(filtered_dataset_path, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df_filtered_active_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcba2b5d-9d23-4119-8ff7-5bd20b25e54b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Create_Datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
